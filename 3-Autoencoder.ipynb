{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e136b3e",
   "metadata": {},
   "source": [
    "# Malicious DNS-over-HTTP Traffic Detection using Autoencoders\n",
    "\n",
    "Welcome to this Jupyter Notebook! We will explore the use of autoencoder for anomaly detection of malicious DoH traffic. The notebook will guide your through loading a DoH traffic dataset, designing an autoencoder, training an autoencoder,  and evaluating their effectiveness in identifying malicious traffic. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8a0970",
   "metadata": {},
   "source": [
    "## Load the dataset\n",
    "\n",
    "We will be using the same training, testing, and evaluation dataset we used for our ML models. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b8e800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Pandas library\n",
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "data_training   = pd.read_csv('data_training.csv')\n",
    "data_testing    = pd.read_csv('data_testing.csv')\n",
    "data_evaluation = pd.read_csv('data_evaluation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8338135",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf27222",
   "metadata": {},
   "source": [
    "## Brief Overview of Autoencoders\n",
    "\n",
    "Autoencoders are deep neural networks that can be used for denoising images, reducing the dimensionality of complex data, and anomaly detection. \n",
    "\n",
    "The main objective of the autoencoder is to recreate statistical features from observed normal DoH traffic with high accuracy on its output. If the input to the autoencoder is a normal DoH TCP connection, the reconstruction error will be small. Otherwise, the reconstruction error will be large. \n",
    "\n",
    "Autoencoders are formed by an encoder, a coder, and a decoder. \n",
    "\n",
    "![image.png](https://miro.medium.com/v2/resize:fit:720/format:webp/1*44eDEuZBEsmG_TCAKRI3Kw@2x.png)\n",
    "Source: [Arden Dertat](https://towardsdatascience.com/applied-deep-learning-part-3-autoencoders-1c083af4d798)\n",
    "\n",
    "The difference between the input and the output can be measured using the mean squared error\n",
    "\n",
    "$$MSE = \\frac{1}{n}(x_i-\\hat{x}_i)^2$$\n",
    "\n",
    "where $x_i$ is the $i$th element in the input sample, and $\\hat{x}_i$ is the $i$ element in the output. If a sample has an $MSE$ larger than a threshold, we determine it is malicious. Otherwise, we determine it is normal. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70514ec7",
   "metadata": {},
   "source": [
    "## The Autoencoder Class\n",
    "The autoencoder is a more complex ML model. Due to the many possible variants, there is no library that provides a standard autoencoder. Instead, we need to build it from scratch. To this end, we use TensorFlow. TensorFlow (along with PyTorch) are the two most common libraries to implement neural networks. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318667a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9e9689f6",
   "metadata": {},
   "source": [
    "TensorFlow expects the class of our model to have tens of standards methods. Luckily, most of them are implemented in a base model class called Model. We only need to define the decoder, encoder, code, and how to glue them together. \n",
    "\n",
    "We define the ```Autoencoder``` class as an inherited class of the ```Model``` class. \n",
    "\n",
    "The ```__init__``` function initializes the autoencoder architecture. This is where we tell TensorFlow how many layers and how many neuros per layer we need. We also specify what type of non-linear functions we should use. \n",
    "\n",
    "The encoder is built with a for loop. The ```layer_output_sizes``` variables contains a list with the number of neurons per layers. The size of the list determines how many layers we have in the encoder. \n",
    "\n",
    "The code (or Embedding layer) is defined with the last element of the ```layer_output_sizes``` list. \n",
    "\n",
    "The decoder is defined as a mirror of the encoder by reversing the order of the elements in ```layer_output_sizes```.\n",
    "\n",
    "The ```call``` function tells TensorFlow in what order the layers should be called. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4dffce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import tensorflow modules\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Lambda, Dense, LeakyReLU, BatchNormalization, Input\n",
    "from tensorflow.keras import Sequential, callbacks, losses\n",
    "from tensorflow.nn import softmax\n",
    "import copy\n",
    "\n",
    "\n",
    "# Ignore errors about CUDA driver. We do not have GPUs :(\n",
    "\n",
    "class Autoencoder(Model):\n",
    "    def __init__(self, layer_output_sizes):\n",
    "          super(Autoencoder, self).__init__()\n",
    "\n",
    "          #self.layer_output_sizes = layer_output_sizes\n",
    "          #workaround to above commented line that does not work for some reason\n",
    "          #self.__dict__['layer_output_sizes'] = layer_output_sizes\n",
    "          layer_output_sizes = copy.deepcopy(layer_output_sizes)\n",
    "\n",
    "          # The Encoder\n",
    "          self.encoder = None\n",
    "          for level in layer_output_sizes[:-1]:\n",
    "              if self.encoder is None:\n",
    "                  self.encoder = Sequential(name='Encoder')\n",
    "                  #self.encoder.add(InputLayer(input_shape=(int(level),)))\n",
    "                  self.encoder.add(Dense(int(level), name='input_layer_encoder'))\n",
    "              else:\n",
    "                  self.encoder.add(Dense(int(level)))\n",
    "                  self.encoder.add(BatchNormalization())\n",
    "                  self.encoder.add(LeakyReLU())\n",
    "\n",
    "          # The Embedding layer of the Encoder\n",
    "          self.encoder.add(Dense(layer_output_sizes[-1]))\n",
    "\n",
    "          # Reverse layer descriptions\n",
    "          layer_output_sizes.reverse()\n",
    "\n",
    "          # The Decoder\n",
    "          self.decoder = None\n",
    "          for level in layer_output_sizes[1:]:\n",
    "              if self.decoder is None:\n",
    "                  self.decoder = Sequential(name='Decoder')\n",
    "                  #self.encoder.add(InputLayer(input_shape=(int(level),)))\n",
    "                  self.decoder.add(Dense(int(level), name='input_layer_decoder'))\n",
    "              else:\n",
    "                  self.decoder.add(Dense(int(level)))\n",
    "                  self.decoder.add(BatchNormalization())\n",
    "                  self.decoder.add(LeakyReLU())\n",
    "\n",
    "    def call(self, x):\n",
    "      encoded = self.encoder(x)\n",
    "      decoded = self.decoder(encoded)\n",
    "      return decoded\n",
    "\n",
    "    def model(self): #for visualization purposes only\n",
    "        x = Input(shape=(16))\n",
    "        return Model(inputs=[x], outputs=self.call(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa867c0",
   "metadata": {},
   "source": [
    "## Initializing an Autoendoer object\n",
    "The class is a template that we can \"instantiate\" everytime we need to create an autoencoder. For now, we only need one. Our autoencdoer will have one hidden layer with 14 neurons. The code layers will have 3 neurons. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebb40c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_architecture = [17,14,3] # the 16 corresponds to the number of features in our dataset. \n",
    "ae = Autoencoder(model_architecture)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055784a5",
   "metadata": {},
   "source": [
    "## Training the autoencoder\n",
    "Training deep neural networks involves two main concepts: loss function and training algorithm. \n",
    "\n",
    "The loss function is a function that measures how well the autoencoder is performing. In our case, we can use the mean squared error described above. This will tell us how well the autoencoder is recreating the DoH traffic data. \n",
    "\n",
    "The training algorithm is the set of steps that we take to update the parameters of the neural network based on how well it is doing. If the loss function is large, we make large changes. If the loss function, is small we make small changes. \n",
    "\n",
    "The most common training algorithm is ADAM optimization, which is based on Stochastic Gradient Descent. The good news is that you do not need to be familiar with this algorithm to train your autoencoder. However, you do need to be familiar with the concept of iterations. \n",
    "\n",
    "The ADAM algorithm updates the paramters of the autoencoder in iterations. Each iteration, the algorithm takes a set of training samples and asks the autoencoder to recreate them. It then updates the parameters according to the calculates the mean squared errors. It continues to pick random samples and updating the parameters until it reaches a maximum number of iterations. \n",
    "\n",
    "The maximum number of iterations is defined by how many times we want the algorithm to use the training data set. Every time the algorithm uses all the training data set is called an epoch. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66565e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the training parameters\n",
    "ae.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "\n",
    "# We create a callback to save the best performing autoencoder\n",
    "# We use the mean squared error (i.e., loss function) to determine if when the model improves\n",
    "# Smaller mean squared errors mean better models\n",
    "cp_callback = callbacks.ModelCheckpoint(filepath='autoencoder_model/', save_weights_only=False, \\\n",
    "                                        monitor='mse', mode='min', save_best_only=True, verbose=1)\n",
    "\n",
    "# Advnced visualization callback with tensorboard\n",
    "# %load_ext tensorboard\n",
    "# import tensorboard\n",
    "# from tensorflow import keras\n",
    "# tensorboard_callback = keras.callbacks.TensorBoard(log_dir='logs/')\n",
    "\n",
    "\n",
    "# Uncomment the following line to use only a few training samples if your training is taking too long.  \n",
    "data_training = data_training.sample(n=500)\n",
    "\n",
    "# Training\n",
    "# The fit function implements the ADAM algorithm for us\n",
    "# We pass data_training twice. First is for the input. Second is to tell the ADAM algorithm how the output should\n",
    "# look like. \n",
    "# validation_split tells the training algorithm what percentage of training data should be saved for validation. \n",
    "# epochs tells the ADAM algorithm when to stop\n",
    "# batch_size tells the ADAM algorithm how manny samples to process per iteration\n",
    "from time import perf_counter\n",
    "tic = perf_counter()#start training timer\n",
    "training_history =  ae.fit(data_training, data_training, validation_split=0.2, \\\n",
    "                           validation_data=None, epochs=3, batch_size=32, verbose=1, \\\n",
    "                           callbacks=[cp_callback,\\\n",
    "                                      #tensorboard_callback\n",
    "                                     ])\n",
    "toc = perf_counter()# stop training timer\n",
    "training_time = toc-tic #calculate total training time\n",
    "\n",
    "print(\"The training time of the Autoencoder was:\")\n",
    "training_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffd6b89",
   "metadata": {},
   "source": [
    "## Deciding whether we need more or less training \n",
    "All machine learning models can suffer from underfitting and over fitting. Underfitting occurs when we do not learn enough from our trianing dataset due to a small number of epochs. Overtraining occurs when we train the model too much. This results in the model learning exactly our training dataset and loosing its ability to genralize its knowledge to useen samples. \n",
    "\n",
    "We can plot the mean squared error for training samples and for testing samples. We want the testing (aka validation) error to be minimum. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc16b08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a dataframe with the training history\n",
    "history_dict = training_history.history\n",
    "history_df= pd.DataFrame(data = history_dict)\n",
    "\n",
    "# Plot the training history\n",
    "fig = history_df.plot.line().get_figure()\n",
    "\n",
    "# Advanced training history graphs\n",
    "# %tensorboard --logdir logs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6105e0c6",
   "metadata": {},
   "source": [
    "## Visuzalizing the autoencoder architecture\n",
    "We initialized the autoencoder. But how does it actually look? We can visualize the layers using several methods. \n",
    "The easiest one is to use this [website](http://alexlenail.me/NN-SVG/index.html) to manually create the architecture. \n",
    "\n",
    "But that doesn't tell us exactly what we are training. \n",
    "\n",
    "Another option is to use the built-in tensorflow visualization library. This will show us what we are actually training. However, it is not very visually appealing. \n",
    "\n",
    "The most visually appealing and accurate option is Tensorboard. Unfortunately, it is also challenging to use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272da7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorflow.keras.utils.plot_model(ae.model(), to_file='ae_arch.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fd93d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %tensorboard --logdir logs # Use at your own risk :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926a164b",
   "metadata": {},
   "source": [
    "### Updating the training parameters\n",
    "Our model is underfitted. It can still learn more from the training dataset without hurting generalization.  We know this because both the training mean squared error (aka loss) and the testing mean squared error (aka validation loss) are both decreasing. \n",
    "\n",
    "Change the epochs and batch size parameters until you see your model overfitting. That is, when the validation loss starts to increase. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7494b001",
   "metadata": {},
   "source": [
    "### Compare the performance to the IF and LOF models. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

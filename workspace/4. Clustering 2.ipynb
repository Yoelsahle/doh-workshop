{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba8a0970",
   "metadata": {},
   "source": [
    "## Load the dataset\n",
    "\n",
    "We will be using the same training, testing, and evaluation dataset we used for our ML models. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46b8e800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Pandas library\n",
    "import pandas as pd\n",
    "import tensorflow\n",
    "\n",
    "# Load the datasets\n",
    "data_training   = pd.read_csv('data_training.csv')\n",
    "data_testing    = pd.read_csv('data_testing.csv')\n",
    "data_evaluation = pd.read_csv('data_evaluation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8338135",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bytes_out</th>\n",
       "      <th>num_pkts_out</th>\n",
       "      <th>bytes_in</th>\n",
       "      <th>num_pkts_in</th>\n",
       "      <th>bytes_ration</th>\n",
       "      <th>num_pkts_ration</th>\n",
       "      <th>time</th>\n",
       "      <th>av_pkt_size_in</th>\n",
       "      <th>av_pkt_size_out</th>\n",
       "      <th>var_pkt_size_in</th>\n",
       "      <th>var_pkt_size_out</th>\n",
       "      <th>median_in</th>\n",
       "      <th>median_out</th>\n",
       "      <th>mindelay</th>\n",
       "      <th>avgdelay</th>\n",
       "      <th>maxdelay</th>\n",
       "      <th>is_doh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.138582</td>\n",
       "      <td>0.161498</td>\n",
       "      <td>0.131184</td>\n",
       "      <td>0.161182</td>\n",
       "      <td>0.315229</td>\n",
       "      <td>0.639265</td>\n",
       "      <td>0.999518</td>\n",
       "      <td>0.131496</td>\n",
       "      <td>0.304707</td>\n",
       "      <td>0.029583</td>\n",
       "      <td>0.087705</td>\n",
       "      <td>0.245385</td>\n",
       "      <td>0.304707</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.281430</td>\n",
       "      <td>0.272718</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000399</td>\n",
       "      <td>0.000422</td>\n",
       "      <td>0.000908</td>\n",
       "      <td>0.000319</td>\n",
       "      <td>0.701939</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.835424</td>\n",
       "      <td>0.436259</td>\n",
       "      <td>0.884305</td>\n",
       "      <td>0.461765</td>\n",
       "      <td>0.857005</td>\n",
       "      <td>0.436259</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002186</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001365</td>\n",
       "      <td>0.001727</td>\n",
       "      <td>0.001638</td>\n",
       "      <td>0.001797</td>\n",
       "      <td>0.390606</td>\n",
       "      <td>0.652174</td>\n",
       "      <td>0.015890</td>\n",
       "      <td>0.165289</td>\n",
       "      <td>0.191003</td>\n",
       "      <td>0.188102</td>\n",
       "      <td>0.178184</td>\n",
       "      <td>0.274746</td>\n",
       "      <td>0.191003</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.409835</td>\n",
       "      <td>0.138663</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.188448</td>\n",
       "      <td>0.212427</td>\n",
       "      <td>0.168130</td>\n",
       "      <td>0.204473</td>\n",
       "      <td>0.297120</td>\n",
       "      <td>0.616570</td>\n",
       "      <td>0.999898</td>\n",
       "      <td>0.134403</td>\n",
       "      <td>0.353895</td>\n",
       "      <td>0.024791</td>\n",
       "      <td>0.087072</td>\n",
       "      <td>0.247910</td>\n",
       "      <td>0.353895</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.217831</td>\n",
       "      <td>0.138832</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.004282</td>\n",
       "      <td>0.005680</td>\n",
       "      <td>0.003919</td>\n",
       "      <td>0.005591</td>\n",
       "      <td>0.302627</td>\n",
       "      <td>0.626398</td>\n",
       "      <td>0.055040</td>\n",
       "      <td>0.092319</td>\n",
       "      <td>0.128817</td>\n",
       "      <td>0.071376</td>\n",
       "      <td>0.092712</td>\n",
       "      <td>0.211345</td>\n",
       "      <td>0.128817</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.443612</td>\n",
       "      <td>0.138246</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>0.000258</td>\n",
       "      <td>0.000269</td>\n",
       "      <td>0.000870</td>\n",
       "      <td>0.000280</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.001199</td>\n",
       "      <td>0.928888</td>\n",
       "      <td>0.449892</td>\n",
       "      <td>0.999292</td>\n",
       "      <td>0.645340</td>\n",
       "      <td>0.938213</td>\n",
       "      <td>0.449892</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.198785</td>\n",
       "      <td>0.134597</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>0.002232</td>\n",
       "      <td>0.002917</td>\n",
       "      <td>0.002287</td>\n",
       "      <td>0.003035</td>\n",
       "      <td>0.336517</td>\n",
       "      <td>0.658009</td>\n",
       "      <td>0.030010</td>\n",
       "      <td>0.110536</td>\n",
       "      <td>0.148537</td>\n",
       "      <td>0.115263</td>\n",
       "      <td>0.128917</td>\n",
       "      <td>0.227173</td>\n",
       "      <td>0.148537</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458287</td>\n",
       "      <td>0.136355</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>0.005953</td>\n",
       "      <td>0.007446</td>\n",
       "      <td>0.005275</td>\n",
       "      <td>0.007867</td>\n",
       "      <td>0.293605</td>\n",
       "      <td>0.673504</td>\n",
       "      <td>0.065986</td>\n",
       "      <td>0.081752</td>\n",
       "      <td>0.205669</td>\n",
       "      <td>0.051024</td>\n",
       "      <td>0.099124</td>\n",
       "      <td>0.202164</td>\n",
       "      <td>0.205669</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.391735</td>\n",
       "      <td>0.138716</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>0.001664</td>\n",
       "      <td>0.002072</td>\n",
       "      <td>0.001927</td>\n",
       "      <td>0.002157</td>\n",
       "      <td>0.378462</td>\n",
       "      <td>0.654545</td>\n",
       "      <td>0.017841</td>\n",
       "      <td>0.159123</td>\n",
       "      <td>0.211977</td>\n",
       "      <td>0.161980</td>\n",
       "      <td>0.162440</td>\n",
       "      <td>0.269388</td>\n",
       "      <td>0.211977</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.383458</td>\n",
       "      <td>0.136355</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2443</th>\n",
       "      <td>0.008853</td>\n",
       "      <td>0.009353</td>\n",
       "      <td>0.011047</td>\n",
       "      <td>0.008525</td>\n",
       "      <td>0.029797</td>\n",
       "      <td>0.676799</td>\n",
       "      <td>0.560160</td>\n",
       "      <td>0.017341</td>\n",
       "      <td>0.141302</td>\n",
       "      <td>0.001428</td>\n",
       "      <td>0.041876</td>\n",
       "      <td>0.026963</td>\n",
       "      <td>0.141302</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.084084</td>\n",
       "      <td>0.135067</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2444 rows Ã— 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      bytes_out  num_pkts_out  bytes_in  num_pkts_in  bytes_ration  \\\n",
       "0      0.138582      0.161498  0.131184     0.161182      0.315229   \n",
       "1      0.000399      0.000422  0.000908     0.000319      0.701939   \n",
       "2      0.001365      0.001727  0.001638     0.001797      0.390606   \n",
       "3      0.188448      0.212427  0.168130     0.204473      0.297120   \n",
       "4      0.004282      0.005680  0.003919     0.005591      0.302627   \n",
       "...         ...           ...       ...          ...           ...   \n",
       "2439   0.000258      0.000269  0.000870     0.000280      1.000000   \n",
       "2440   0.002232      0.002917  0.002287     0.003035      0.336517   \n",
       "2441   0.005953      0.007446  0.005275     0.007867      0.293605   \n",
       "2442   0.001664      0.002072  0.001927     0.002157      0.378462   \n",
       "2443   0.008853      0.009353  0.011047     0.008525      0.029797   \n",
       "\n",
       "      num_pkts_ration      time  av_pkt_size_in  av_pkt_size_out  \\\n",
       "0            0.639265  0.999518        0.131496         0.304707   \n",
       "1            0.444444  0.000018        0.835424         0.436259   \n",
       "2            0.652174  0.015890        0.165289         0.191003   \n",
       "3            0.616570  0.999898        0.134403         0.353895   \n",
       "4            0.626398  0.055040        0.092319         0.128817   \n",
       "...               ...       ...             ...              ...   \n",
       "2439         0.583333  0.001199        0.928888         0.449892   \n",
       "2440         0.658009  0.030010        0.110536         0.148537   \n",
       "2441         0.673504  0.065986        0.081752         0.205669   \n",
       "2442         0.654545  0.017841        0.159123         0.211977   \n",
       "2443         0.676799  0.560160        0.017341         0.141302   \n",
       "\n",
       "      var_pkt_size_in  var_pkt_size_out  median_in  median_out  mindelay  \\\n",
       "0            0.029583          0.087705   0.245385    0.304707       0.0   \n",
       "1            0.884305          0.461765   0.857005    0.436259       0.0   \n",
       "2            0.188102          0.178184   0.274746    0.191003       0.0   \n",
       "3            0.024791          0.087072   0.247910    0.353895       0.0   \n",
       "4            0.071376          0.092712   0.211345    0.128817       0.0   \n",
       "...               ...               ...        ...         ...       ...   \n",
       "2439         0.999292          0.645340   0.938213    0.449892       0.0   \n",
       "2440         0.115263          0.128917   0.227173    0.148537       0.0   \n",
       "2441         0.051024          0.099124   0.202164    0.205669       0.0   \n",
       "2442         0.161980          0.162440   0.269388    0.211977       0.0   \n",
       "2443         0.001428          0.041876   0.026963    0.141302       0.0   \n",
       "\n",
       "      avgdelay  maxdelay  is_doh  \n",
       "0     0.281430  0.272718       1  \n",
       "1     0.002186  0.003906       1  \n",
       "2     0.409835  0.138663       1  \n",
       "3     0.217831  0.138832       1  \n",
       "4     0.443612  0.138246       1  \n",
       "...        ...       ...     ...  \n",
       "2439  0.198785  0.134597       1  \n",
       "2440  0.458287  0.136355       1  \n",
       "2441  0.391735  0.138716       1  \n",
       "2442  0.383458  0.136355       1  \n",
       "2443  0.084084  0.135067      -1  \n",
       "\n",
       "[2444 rows x 17 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f7fb4a",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9653872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.3476482617586912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yoelwoldeyes/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load your dataset\n",
    "data = pd.read_csv('data_training.csv')\n",
    "\n",
    "# Splitting the data into features (X) and target variable (y)\n",
    "X = data_training.drop('is_doh', axis=1)\n",
    "y = data_training['is_doh']\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Using KMeans for classification\n",
    "kmeans = KMeans(n_clusters=2, random_state=42)  # Assuming 2 clusters for benign and malicious\n",
    "kmeans.fit(X_train)\n",
    "\n",
    "# Getting predictions on the test set\n",
    "y_pred = kmeans.predict(X_test)\n",
    "\n",
    "# Converting the cluster labels to match the target variable\n",
    "# Assuming the majority cluster corresponds to benign (1) and the minority to malicious (-1)\n",
    "majority_cluster = max(set(y_pred), key=list(y_pred).count)\n",
    "predicted_labels = [1 if cluster == majority_cluster else -1 for cluster in y_pred]\n",
    "\n",
    "# Evaluating the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, predicted_labels)\n",
    "print(f\"Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa1f218f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9959100204498977\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          -1       1.00      0.98      0.99       111\n",
      "           1       0.99      1.00      1.00       378\n",
      "\n",
      "    accuracy                           1.00       489\n",
      "   macro avg       1.00      0.99      0.99       489\n",
      "weighted avg       1.00      1.00      1.00       489\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X = data_training.drop('is_doh', axis=1)\n",
    "y = data_training['is_doh']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create a logistic regression model\n",
    "model = LogisticRegression(random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "report = classification_report(y_test, predictions)\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print('Classification Report:\\n', report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be8a154b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /Users/yoelwoldeyes/anaconda3/lib/python3.11/site-packages (1.3.0)\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit-learn-1.3.2.tar.gz (7.5 MB)\n",
      "  Installing build dependencies ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31mÃ—\u001b[0m \u001b[32mpip subprocess to install build dependencies\u001b[0m did not run successfully.\n",
      "  \u001b[31mâ”‚\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31mâ•°â”€>\u001b[0m \u001b[31m[80 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m Ignoring numpy: markers 'python_version == \"3.10\" and platform_system == \"Windows\" and platform_python_implementation != \"PyPy\"' don't match your environment\n",
      "  \u001b[31m   \u001b[0m Collecting setuptools\n",
      "  \u001b[31m   \u001b[0m   Obtaining dependency information for setuptools from https://files.pythonhosted.org/packages/bb/26/7945080113158354380a12ce26873dd6c1ebd88d47f5bc24e2c5bb38c16a/setuptools-68.2.2-py3-none-any.whl.metadata\n",
      "  \u001b[31m   \u001b[0m   Using cached setuptools-68.2.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "  \u001b[31m   \u001b[0m Collecting wheel\n",
      "  \u001b[31m   \u001b[0m   Obtaining dependency information for wheel from https://files.pythonhosted.org/packages/fa/7f/4c07234086edbce4a0a446209dc0cb08a19bb206a3ea53b2f56a403f983b/wheel-0.41.3-py3-none-any.whl.metadata\n",
      "  \u001b[31m   \u001b[0m   Using cached wheel-0.41.3-py3-none-any.whl.metadata (2.2 kB)\n",
      "  \u001b[31m   \u001b[0m Collecting Cython<3.0,>=0.29.33\n",
      "  \u001b[31m   \u001b[0m   Obtaining dependency information for Cython<3.0,>=0.29.33 from https://files.pythonhosted.org/packages/3f/d6/9eed523aeaca42acbaa3e6d3850edae780dc7f8da9df1bf6a2ceb851839c/Cython-0.29.36-py2.py3-none-any.whl.metadata\n",
      "  \u001b[31m   \u001b[0m   Using cached Cython-0.29.36-py2.py3-none-any.whl.metadata (3.1 kB)\n",
      "  \u001b[31m   \u001b[0m Collecting oldest-supported-numpy\n",
      "  \u001b[31m   \u001b[0m   Obtaining dependency information for oldest-supported-numpy from https://files.pythonhosted.org/packages/f2/a0/e3ddf322257d6d8798b5f08b5160eb697e0cd7f61cb70ad674aabae18051/oldest_supported_numpy-2023.10.25-py3-none-any.whl.metadata\n",
      "  \u001b[31m   \u001b[0m   Using cached oldest_supported_numpy-2023.10.25-py3-none-any.whl.metadata (9.8 kB)\n",
      "  \u001b[31m   \u001b[0m Collecting scipy>=1.5.0\n",
      "  \u001b[31m   \u001b[0m   Using cached scipy-1.11.3.tar.gz (56.3 MB)\n",
      "  \u001b[31m   \u001b[0m   Installing build dependencies: started\n",
      "  \u001b[31m   \u001b[0m   Installing build dependencies: finished with status 'done'\n",
      "  \u001b[31m   \u001b[0m   Getting requirements to build wheel: started\n",
      "  \u001b[31m   \u001b[0m   Getting requirements to build wheel: finished with status 'done'\n",
      "  \u001b[31m   \u001b[0m   Preparing metadata (pyproject.toml): started\n",
      "  \u001b[31m   \u001b[0m   Preparing metadata (pyproject.toml): finished with status 'error'\n",
      "  \u001b[31m   \u001b[0m   \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m   \u001b[31mÃ—\u001b[0m \u001b[32mPreparing metadata \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
      "  \u001b[31m   \u001b[0m   \u001b[31mâ”‚\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m   \u001b[0m   \u001b[31mâ•°â”€>\u001b[0m \u001b[31m[44 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m \u001b[36m\u001b[1m+ meson setup /private/var/folders/hy/bsxb0cbn217_9l5m13nzbsc80000gn/T/pip-install-q96088jx/scipy_252d180922b749d9bc53addf60a224c6 /private/var/folders/hy/bsxb0cbn217_9l5m13nzbsc80000gn/T/pip-install-q96088jx/scipy_252d180922b749d9bc53addf60a224c6/.mesonpy-qe7nmnsk -Dbuildtype=release -Db_ndebug=if-release -Db_vscrt=md --native-file=/private/var/folders/hy/bsxb0cbn217_9l5m13nzbsc80000gn/T/pip-install-q96088jx/scipy_252d180922b749d9bc53addf60a224c6/.mesonpy-qe7nmnsk/meson-python-native-file.ini\u001b[0m\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m The Meson build system\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m Version: 1.2.3\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m Source dir: /private/var/folders/hy/bsxb0cbn217_9l5m13nzbsc80000gn/T/pip-install-q96088jx/scipy_252d180922b749d9bc53addf60a224c6\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m Build dir: /private/var/folders/hy/bsxb0cbn217_9l5m13nzbsc80000gn/T/pip-install-q96088jx/scipy_252d180922b749d9bc53addf60a224c6/.mesonpy-qe7nmnsk\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m Build type: native build\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m Project name: SciPy\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m Project version: 1.11.3\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m C compiler for the host machine: cc (clang 12.0.5 \"Apple clang version 12.0.5 (clang-1205.0.22.11)\")\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m C linker for the host machine: cc ld64 650.9\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m C++ compiler for the host machine: c++ (clang 12.0.5 \"Apple clang version 12.0.5 (clang-1205.0.22.11)\")\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m C++ linker for the host machine: c++ ld64 650.9\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m Cython compiler for the host machine: cython (cython 0.29.36)\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m Host machine cpu family: aarch64\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m Host machine cpu: aarch64\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m Program python found: YES (/Users/yoelwoldeyes/anaconda3/bin/python)\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m Did not find pkg-config by name 'pkg-config'\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m Found Pkg-config: NO\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m Run-time dependency python found: YES 3.11\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m Program cython found: YES (/private/var/folders/hy/bsxb0cbn217_9l5m13nzbsc80000gn/T/pip-build-env-lzjm4q1m/overlay/bin/cython)\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m Compiler for C supports arguments -Wno-unused-but-set-variable: NO\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m Compiler for C supports arguments -Wno-unused-function: YES\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m Compiler for C supports arguments -Wno-conversion: YES\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m Compiler for C supports arguments -Wno-misleading-indentation: YES\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m Library m found: YES\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m ../meson.build:82:0: ERROR: Unknown compiler(s): [['gfortran'], ['flang'], ['nvfortran'], ['pgfortran'], ['ifort'], ['ifx'], ['g95']]\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m The following exception(s) were encountered:\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m Running `gfortran --version` gave \"[Errno 2] No such file or directory: 'gfortran'\"\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m Running `gfortran -V` gave \"[Errno 2] No such file or directory: 'gfortran'\"\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m Running `flang --version` gave \"[Errno 2] No such file or directory: 'flang'\"\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m Running `flang -V` gave \"[Errno 2] No such file or directory: 'flang'\"\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m Running `nvfortran --version` gave \"[Errno 2] No such file or directory: 'nvfortran'\"\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m Running `nvfortran -V` gave \"[Errno 2] No such file or directory: 'nvfortran'\"\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m Running `pgfortran --version` gave \"[Errno 2] No such file or directory: 'pgfortran'\"\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m Running `pgfortran -V` gave \"[Errno 2] No such file or directory: 'pgfortran'\"\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m Running `ifort --version` gave \"[Errno 2] No such file or directory: 'ifort'\"\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m Running `ifort -V` gave \"[Errno 2] No such file or directory: 'ifort'\"\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m Running `ifx --version` gave \"[Errno 2] No such file or directory: 'ifx'\"\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m Running `ifx -V` gave \"[Errno 2] No such file or directory: 'ifx'\"\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m Running `g95 --version` gave \"[Errno 2] No such file or directory: 'g95'\"\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m Running `g95 -V` gave \"[Errno 2] No such file or directory: 'g95'\"\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m A full log can be found at /private/var/folders/hy/bsxb0cbn217_9l5m13nzbsc80000gn/T/pip-install-q96088jx/scipy_252d180922b749d9bc53addf60a224c6/.mesonpy-qe7nmnsk/meson-logs/meson-log.txt\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m   \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  \u001b[31m   \u001b[0m \u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m \u001b[31mÃ—\u001b[0m Encountered error while generating package metadata.\n",
      "  \u001b[31m   \u001b[0m \u001b[31mâ•°â”€>\u001b[0m See above for output.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m \u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
      "  \u001b[31m   \u001b[0m \u001b[1;36mhint\u001b[0m: See above for details.\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "\n",
      "\u001b[31mÃ—\u001b[0m \u001b[32mpip subprocess to install build dependencies\u001b[0m did not run successfully.\n",
      "\u001b[31mâ”‚\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "\u001b[31mâ•°â”€>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[?25hRequirement already satisfied: imbalanced-learn in /Users/yoelwoldeyes/anaconda3/lib/python3.11/site-packages (0.11.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /Users/yoelwoldeyes/anaconda3/lib/python3.11/site-packages (from imbalanced-learn) (1.23.5)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /Users/yoelwoldeyes/anaconda3/lib/python3.11/site-packages (from imbalanced-learn) (1.10.1)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in /Users/yoelwoldeyes/anaconda3/lib/python3.11/site-packages (from imbalanced-learn) (1.3.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/yoelwoldeyes/anaconda3/lib/python3.11/site-packages (from imbalanced-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/yoelwoldeyes/anaconda3/lib/python3.11/site-packages (from imbalanced-learn) (2.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade scikit-learn\n",
    "!pip install --upgrade imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebb6b21d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Labels for New Data:\n",
      "0      -1\n",
      "1       1\n",
      "2      -1\n",
      "3      -1\n",
      "4      -1\n",
      "       ..\n",
      "2439    1\n",
      "2440   -1\n",
      "2441   -1\n",
      "2442   -1\n",
      "2443   -1\n",
      "Name: Label, Length: 2444, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yoelwoldeyes/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Load the training dataset\n",
    "data_training = pd.read_csv('data_training.csv')\n",
    "\n",
    "# Prepare the data\n",
    "X_train = data_training.drop('is_doh', axis=1)\n",
    "y_train = data_training['is_doh']\n",
    "\n",
    "# Balance the dataset using SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Perform K-Means Clustering\n",
    "kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "clusters = kmeans.fit_predict(X_resampled)\n",
    "\n",
    "# Assign Labels to Clusters based on the majority class\n",
    "cluster_labels = pd.DataFrame({'Cluster': clusters, 'Label': y_resampled})\n",
    "cluster_majority_labels = cluster_labels.groupby('Cluster')['Label'].apply(lambda x: x.mode()[0]).reset_index()\n",
    "\n",
    "# Load the new data for prediction\n",
    "new_data = pd.read_csv('data_training.csv')  # Use the same training data for illustration purposes\n",
    "new_data_features = new_data.drop('is_doh', axis=1)  # Exclude the target variable from features\n",
    "\n",
    "# Predict labels for new data\n",
    "new_data_clusters = kmeans.predict(new_data_features)\n",
    "new_data_labels = pd.merge(pd.DataFrame({'Cluster': new_data_clusters}), cluster_majority_labels, how='left', on='Cluster')['Label']\n",
    "\n",
    "# Display the predicted labels for the new data\n",
    "print(\"Predicted Labels for New Data:\")\n",
    "print(new_data_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b939df8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the training dataset\n",
    "data_training = pd.read_csv('data_training.csv')\n",
    "\n",
    "benign_samples = data_training[data_training['is_doh'] == 1].sample(n=500, random_state=42)\n",
    "malicious_samples = data_training[data_training['is_doh'] == -1].sample(n=500, random_state=42)\n",
    "balanced_training_dataset = pd.concat([benign_samples, malicious_samples], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "138e0c68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bytes_out</th>\n",
       "      <th>num_pkts_out</th>\n",
       "      <th>bytes_in</th>\n",
       "      <th>num_pkts_in</th>\n",
       "      <th>bytes_ration</th>\n",
       "      <th>num_pkts_ration</th>\n",
       "      <th>time</th>\n",
       "      <th>av_pkt_size_in</th>\n",
       "      <th>av_pkt_size_out</th>\n",
       "      <th>var_pkt_size_in</th>\n",
       "      <th>var_pkt_size_out</th>\n",
       "      <th>median_in</th>\n",
       "      <th>median_out</th>\n",
       "      <th>mindelay</th>\n",
       "      <th>avgdelay</th>\n",
       "      <th>maxdelay</th>\n",
       "      <th>is_doh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000399</td>\n",
       "      <td>0.000422</td>\n",
       "      <td>0.000908</td>\n",
       "      <td>0.000319</td>\n",
       "      <td>0.701742</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.835148</td>\n",
       "      <td>0.436259</td>\n",
       "      <td>0.884483</td>\n",
       "      <td>0.461765</td>\n",
       "      <td>0.856765</td>\n",
       "      <td>0.436259</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002138</td>\n",
       "      <td>0.003835</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.977946</td>\n",
       "      <td>0.979774</td>\n",
       "      <td>0.949626</td>\n",
       "      <td>0.979233</td>\n",
       "      <td>0.323426</td>\n",
       "      <td>0.640292</td>\n",
       "      <td>0.999589</td>\n",
       "      <td>0.185586</td>\n",
       "      <td>0.542102</td>\n",
       "      <td>0.029239</td>\n",
       "      <td>0.093932</td>\n",
       "      <td>0.292382</td>\n",
       "      <td>0.542102</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.046360</td>\n",
       "      <td>0.138841</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.002506</td>\n",
       "      <td>0.003377</td>\n",
       "      <td>0.002495</td>\n",
       "      <td>0.003594</td>\n",
       "      <td>0.327464</td>\n",
       "      <td>0.674157</td>\n",
       "      <td>0.038231</td>\n",
       "      <td>0.089938</td>\n",
       "      <td>0.109479</td>\n",
       "      <td>0.087309</td>\n",
       "      <td>0.110635</td>\n",
       "      <td>0.209276</td>\n",
       "      <td>0.109479</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.498552</td>\n",
       "      <td>0.138485</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000399</td>\n",
       "      <td>0.000422</td>\n",
       "      <td>0.000908</td>\n",
       "      <td>0.000319</td>\n",
       "      <td>0.701742</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.835148</td>\n",
       "      <td>0.436259</td>\n",
       "      <td>0.884483</td>\n",
       "      <td>0.461765</td>\n",
       "      <td>0.856765</td>\n",
       "      <td>0.436259</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006220</td>\n",
       "      <td>0.011142</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.010938</td>\n",
       "      <td>0.007737</td>\n",
       "      <td>0.011502</td>\n",
       "      <td>0.300401</td>\n",
       "      <td>0.671329</td>\n",
       "      <td>0.110992</td>\n",
       "      <td>0.082495</td>\n",
       "      <td>0.174937</td>\n",
       "      <td>0.043909</td>\n",
       "      <td>0.090407</td>\n",
       "      <td>0.202809</td>\n",
       "      <td>0.174937</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.449630</td>\n",
       "      <td>0.138672</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>0.073514</td>\n",
       "      <td>0.068100</td>\n",
       "      <td>0.083841</td>\n",
       "      <td>0.081398</td>\n",
       "      <td>0.027626</td>\n",
       "      <td>0.892048</td>\n",
       "      <td>0.091712</td>\n",
       "      <td>0.011756</td>\n",
       "      <td>0.193594</td>\n",
       "      <td>0.000382</td>\n",
       "      <td>0.027558</td>\n",
       "      <td>0.021433</td>\n",
       "      <td>0.193594</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.135094</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>0.081866</td>\n",
       "      <td>0.076195</td>\n",
       "      <td>0.093762</td>\n",
       "      <td>0.093134</td>\n",
       "      <td>0.027749</td>\n",
       "      <td>0.912293</td>\n",
       "      <td>0.365234</td>\n",
       "      <td>0.011267</td>\n",
       "      <td>0.191407</td>\n",
       "      <td>0.000343</td>\n",
       "      <td>0.027538</td>\n",
       "      <td>0.020949</td>\n",
       "      <td>0.191407</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005614</td>\n",
       "      <td>0.134695</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>0.031754</td>\n",
       "      <td>0.029701</td>\n",
       "      <td>0.036427</td>\n",
       "      <td>0.034778</td>\n",
       "      <td>0.027717</td>\n",
       "      <td>0.872964</td>\n",
       "      <td>0.030085</td>\n",
       "      <td>0.012122</td>\n",
       "      <td>0.189909</td>\n",
       "      <td>0.000511</td>\n",
       "      <td>0.028259</td>\n",
       "      <td>0.021795</td>\n",
       "      <td>0.189909</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001220</td>\n",
       "      <td>0.008754</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>0.012162</td>\n",
       "      <td>0.011651</td>\n",
       "      <td>0.011293</td>\n",
       "      <td>0.013340</td>\n",
       "      <td>0.022272</td>\n",
       "      <td>0.851233</td>\n",
       "      <td>0.364766</td>\n",
       "      <td>0.007901</td>\n",
       "      <td>0.181239</td>\n",
       "      <td>0.000615</td>\n",
       "      <td>0.029627</td>\n",
       "      <td>0.017616</td>\n",
       "      <td>0.181239</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.038244</td>\n",
       "      <td>0.134801</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>0.050358</td>\n",
       "      <td>0.051854</td>\n",
       "      <td>0.063058</td>\n",
       "      <td>0.044587</td>\n",
       "      <td>0.030305</td>\n",
       "      <td>0.641556</td>\n",
       "      <td>0.238687</td>\n",
       "      <td>0.019831</td>\n",
       "      <td>0.148252</td>\n",
       "      <td>0.000794</td>\n",
       "      <td>0.038937</td>\n",
       "      <td>0.029429</td>\n",
       "      <td>0.148252</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006683</td>\n",
       "      <td>0.135005</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     bytes_out  num_pkts_out  bytes_in  num_pkts_in  bytes_ration  \\\n",
       "0     0.000399      0.000422  0.000908     0.000319      0.701742   \n",
       "1     0.977946      0.979774  0.949626     0.979233      0.323426   \n",
       "2     0.002506      0.003377  0.002495     0.003594      0.327464   \n",
       "3     0.000399      0.000422  0.000908     0.000319      0.701742   \n",
       "4     0.008547      0.010938  0.007737     0.011502      0.300401   \n",
       "..         ...           ...       ...          ...           ...   \n",
       "995   0.073514      0.068100  0.083841     0.081398      0.027626   \n",
       "996   0.081866      0.076195  0.093762     0.093134      0.027749   \n",
       "997   0.031754      0.029701  0.036427     0.034778      0.027717   \n",
       "998   0.012162      0.011651  0.011293     0.013340      0.022272   \n",
       "999   0.050358      0.051854  0.063058     0.044587      0.030305   \n",
       "\n",
       "     num_pkts_ration      time  av_pkt_size_in  av_pkt_size_out  \\\n",
       "0           0.444444  0.000018        0.835148         0.436259   \n",
       "1           0.640292  0.999589        0.185586         0.542102   \n",
       "2           0.674157  0.038231        0.089938         0.109479   \n",
       "3           0.444444  0.000051        0.835148         0.436259   \n",
       "4           0.671329  0.110992        0.082495         0.174937   \n",
       "..               ...       ...             ...              ...   \n",
       "995         0.892048  0.091712        0.011756         0.193594   \n",
       "996         0.912293  0.365234        0.011267         0.191407   \n",
       "997         0.872964  0.030085        0.012122         0.189909   \n",
       "998         0.851233  0.364766        0.007901         0.181239   \n",
       "999         0.641556  0.238687        0.019831         0.148252   \n",
       "\n",
       "     var_pkt_size_in  var_pkt_size_out  median_in  median_out  mindelay  \\\n",
       "0           0.884483          0.461765   0.856765    0.436259       0.0   \n",
       "1           0.029239          0.093932   0.292382    0.542102       0.0   \n",
       "2           0.087309          0.110635   0.209276    0.109479       0.0   \n",
       "3           0.884483          0.461765   0.856765    0.436259       0.0   \n",
       "4           0.043909          0.090407   0.202809    0.174937       0.0   \n",
       "..               ...               ...        ...         ...       ...   \n",
       "995         0.000382          0.027558   0.021433    0.193594       0.0   \n",
       "996         0.000343          0.027538   0.020949    0.191407       0.0   \n",
       "997         0.000511          0.028259   0.021795    0.189909       0.0   \n",
       "998         0.000615          0.029627   0.017616    0.181239       0.0   \n",
       "999         0.000794          0.038937   0.029429    0.148252       0.0   \n",
       "\n",
       "     avgdelay  maxdelay  is_doh  \n",
       "0    0.002138  0.003835       1  \n",
       "1    0.046360  0.138841       1  \n",
       "2    0.498552  0.138485       1  \n",
       "3    0.006220  0.011142       1  \n",
       "4    0.449630  0.138672       1  \n",
       "..        ...       ...     ...  \n",
       "995  0.001600  0.135094      -1  \n",
       "996  0.005614  0.134695      -1  \n",
       "997  0.001220  0.008754      -1  \n",
       "998  0.038244  0.134801      -1  \n",
       "999  0.006683  0.135005      -1  \n",
       "\n",
       "[1000 rows x 17 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balanced_training_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "181fe6a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of k-Means Clustering for Classification: 0.243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yoelwoldeyes/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Assuming 'X' is the feature matrix and 'y' is the ground truth labels\n",
    "X = balanced_training_dataset.drop(columns=['is_doh'])\n",
    "y = balanced_training_dataset['is_doh']\n",
    "\n",
    "# Fit k-Means\n",
    "kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "kmeans.fit(X)\n",
    "\n",
    "# Predict labels\n",
    "predictions = kmeans.labels_\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y, predictions)\n",
    "print(f\"Accuracy of k-Means Clustering for Classification: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6bfe6c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import tensorflow modules\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Lambda, Dense, LeakyReLU, BatchNormalization, Input\n",
    "from tensorflow.keras import Sequential, callbacks, losses\n",
    "from tensorflow.nn import softmax\n",
    "import copy\n",
    "\n",
    "\n",
    "# Ignore errors about CUDA driver. We do not have GPUs :(\n",
    "\n",
    "class Autoencoder(Model):\n",
    "    def __init__(self, layer_output_sizes):\n",
    "          super(Autoencoder, self).__init__()\n",
    "\n",
    "          #self.layer_output_sizes = layer_output_sizes\n",
    "          #workaround to above commented line that does not work for some reason\n",
    "          #self.__dict__['layer_output_sizes'] = layer_output_sizes\n",
    "          layer_output_sizes = copy.deepcopy(layer_output_sizes)\n",
    "\n",
    "          # The Encoder\n",
    "          self.encoder = None\n",
    "          for level in layer_output_sizes[:-1]:\n",
    "              if self.encoder is None:\n",
    "                  self.encoder = Sequential(name='Encoder')\n",
    "                  #self.encoder.add(InputLayer(input_shape=(int(level),)))\n",
    "                  self.encoder.add(Dense(int(level), name='input_layer_encoder'))\n",
    "              else:\n",
    "                  self.encoder.add(Dense(int(level)))\n",
    "                  self.encoder.add(BatchNormalization())\n",
    "                  self.encoder.add(LeakyReLU())\n",
    "\n",
    "          # The Embedding layer of the Encoder\n",
    "          self.encoder.add(Dense(layer_output_sizes[-1]))\n",
    "\n",
    "          # Reverse layer descriptions\n",
    "          layer_output_sizes.reverse()\n",
    "            \n",
    "          \n",
    "          # The Decoder\n",
    "          self.decoder = None\n",
    "          for level in layer_output_sizes[1:]:\n",
    "              if self.decoder is None:\n",
    "                  self.decoder = Sequential(name='Decoder')\n",
    "                  #self.encoder.add(InputLayer(input_shape=(int(level),)))\n",
    "                  self.decoder.add(Dense(int(level), name='input_layer_decoder'))\n",
    "              else:\n",
    "                  self.decoder.add(Dense(int(level)))\n",
    "                  self.decoder.add(BatchNormalization())\n",
    "                  self.decoder.add(LeakyReLU())\n",
    "\n",
    "    def call(self, x):\n",
    "      encoded = self.encoder(x)\n",
    "      decoded = self.decoder(encoded)\n",
    "      return decoded\n",
    "\n",
    "    def model(self): #for visualization purposes only\n",
    "        x = Input(shape=(17))\n",
    "        return Model(inputs=[x], outputs=self.call(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65c91aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_architecture = [17,14,3] # the 16 corresponds to the number of features in our dataset. \n",
    "ae = Autoencoder(model_architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0fd8b161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-16 14:28:46.951923: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/13 [=>............................] - ETA: 4s - loss: 0.5530WARNING:tensorflow:Can save best model only with mse available, skipping.\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.4953 - val_loss: 0.1918\n",
      "Epoch 2/3\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.4314WARNING:tensorflow:Can save best model only with mse available, skipping.\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.4145 - val_loss: 0.1750\n",
      "Epoch 3/3\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 0.3655WARNING:tensorflow:Can save best model only with mse available, skipping.\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.3568 - val_loss: 0.1701\n",
      "The training time of the Autoencoder was:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6468397499993443"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the training parameters\n",
    "ae.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "\n",
    "# We create a callback to save the best performing autoencoder\n",
    "# We use the mean squared error (i.e., loss function) to determine if when the model improves\n",
    "# Smaller mean squared errors mean better models\n",
    "cp_callback = callbacks.ModelCheckpoint(filepath='autoencoder_model/', save_weights_only=False, \\\n",
    "                                        monitor='mse', mode='min', save_best_only=True, verbose=1)\n",
    "\n",
    "# Advnced visualization callback with tensorboard\n",
    "# %load_ext tensorboard\n",
    "# import tensorboard\n",
    "# from tensorflow import keras\n",
    "# tensorboard_callback = keras.callbacks.TensorBoard(log_dir='logs/')\n",
    "\n",
    "\n",
    "# Uncomment the following line to use only a few training samples if your training is taking too long.  \n",
    "data_training = data_training.sample(n=500)\n",
    "\n",
    "# Training\n",
    "# The fit function implements the ADAM algorithm for us\n",
    "# We pass data_training twice. First is for the input. Second is to tell the ADAM algorithm how the output should\n",
    "# look like. \n",
    "# validation_split tells the training algorithm what percentage of training data should be saved for validation. \n",
    "# epochs tells the ADAM algorithm when to stop\n",
    "# batch_size tells the ADAM algorithm how manny samples to process per iteration\n",
    "from time import perf_counter\n",
    "tic = perf_counter()#start training timer\n",
    "training_history =  ae.fit(data_training, data_training, validation_split=0.2, \\\n",
    "                           validation_data=None, epochs=3, batch_size=32, verbose=1, \\\n",
    "                           callbacks=[cp_callback,\\\n",
    "                                      #tensorboard_callback\n",
    "                                     ])\n",
    "toc = perf_counter()# stop training timer\n",
    "training_time = toc-tic #calculate total training time\n",
    "\n",
    "print(\"The training time of the Autoencoder was:\")\n",
    "training_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c748fc51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "77/77 [==============================] - 0s 616us/step - loss: 0.3291\n",
      "Epoch 2/10\n",
      "77/77 [==============================] - 0s 484us/step - loss: 0.1941\n",
      "Epoch 3/10\n",
      "77/77 [==============================] - 0s 491us/step - loss: 0.1555\n",
      "Epoch 4/10\n",
      "77/77 [==============================] - 0s 476us/step - loss: 0.1061\n",
      "Epoch 5/10\n",
      "77/77 [==============================] - 0s 483us/step - loss: 0.0822\n",
      "Epoch 6/10\n",
      "77/77 [==============================] - 0s 492us/step - loss: 0.0663\n",
      "Epoch 7/10\n",
      "77/77 [==============================] - 0s 522us/step - loss: 0.0527\n",
      "Epoch 8/10\n",
      "77/77 [==============================] - 0s 531us/step - loss: 0.0464\n",
      "Epoch 9/10\n",
      "77/77 [==============================] - 0s 514us/step - loss: 0.0391\n",
      "Epoch 10/10\n",
      "77/77 [==============================] - 0s 522us/step - loss: 0.0334\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1686f3590>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_architecture = [16, 14, 3]\n",
    "\n",
    "# Instantiate the Autoencoder with the updated architecture\n",
    "ae = Autoencoder(model_architecture)\n",
    "\n",
    "# Compile and fit the Autoencoder\n",
    "ae.compile(optimizer='adam', loss='mse')\n",
    "ae.fit(X_all, X_all, epochs=10, batch_size=32, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "33ef6e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77/77 [==============================] - 0s 278us/step\n"
     ]
    }
   ],
   "source": [
    "# Obtain 3-dimensional representations (embeddings or code)\n",
    "embeddings = ae.encoder.predict(X_all)\n",
    "# 'embeddings' is a 2D array with 3 columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cf0b41f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "16/16 [==============================] - 1s 2ms/step - loss: 0.0292\n",
      "Epoch 2/10\n",
      "16/16 [==============================] - 0s 663us/step - loss: 0.0278\n",
      "Epoch 3/10\n",
      "16/16 [==============================] - 0s 578us/step - loss: 0.0260\n",
      "Epoch 4/10\n",
      "16/16 [==============================] - 0s 609us/step - loss: 0.0236\n",
      "Epoch 5/10\n",
      "16/16 [==============================] - 0s 658us/step - loss: 0.0215\n",
      "Epoch 6/10\n",
      "16/16 [==============================] - 0s 601us/step - loss: 0.0223\n",
      "Epoch 7/10\n",
      "16/16 [==============================] - 0s 634us/step - loss: 0.0190\n",
      "Epoch 8/10\n",
      "16/16 [==============================] - 0s 624us/step - loss: 0.0188\n",
      "Epoch 9/10\n",
      "16/16 [==============================] - 0s 592us/step - loss: 0.0164\n",
      "Epoch 10/10\n",
      "16/16 [==============================] - 0s 601us/step - loss: 0.0154\n",
      "16/16 [==============================] - 0s 310us/step\n",
      "Accuracy of k-Means Clustering on Embeddings: 0.63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yoelwoldeyes/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    }
   ],
   "source": [
    "# Build a New Dataset for Dimensionality Reduction\n",
    "\n",
    "# 'X_all' is the entire feature matrix of 'data_training'\n",
    "X_all = data_training.drop(columns=['is_doh'])\n",
    "\n",
    "# Train the Autoencoder\n",
    "\n",
    "# 'ae' is the Autoencoder model\n",
    "ae.compile(optimizer='adam', loss='mse')\n",
    "ae.fit(X_all, X_all, epochs=10, batch_size=32, verbose=1)\n",
    "\n",
    "# Get 3-Dimensional Representations\n",
    "\n",
    "# Obtain 3-dimensional representations (embeddings or code)\n",
    "embeddings = ae.encoder.predict(X_all)\n",
    "# 'embeddings' is a 2D array with 3 columns\n",
    "\n",
    "# Run k-Means Clustering on Embeddings\n",
    "\n",
    "# Fit k-Means on embeddings\n",
    "kmeans_on_embeddings = KMeans(n_clusters=2, random_state=42)\n",
    "kmeans_on_embeddings.fit(embeddings)\n",
    "\n",
    "# Predict labels\n",
    "predictions_on_embeddings = kmeans_on_embeddings.labels_\n",
    "\n",
    "# Evaluate accuracy\n",
    "y_binary = (data_training['is_doh'] == 1).astype(int)\n",
    "accuracy_on_embeddings = accuracy_score(y_binary, predictions_on_embeddings)\n",
    "print(f\"Accuracy of k-Means Clustering on Embeddings: {accuracy_on_embeddings}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe930377",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
